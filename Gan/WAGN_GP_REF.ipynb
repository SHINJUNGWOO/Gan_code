{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from functools import partial\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "\n",
    "    def __init__(self\n",
    "        , input_dim\n",
    "        , discriminator_conv_filters\n",
    "        , discriminator_conv_kernel_size\n",
    "        , discriminator_conv_strides\n",
    "        , generator_initial_dense_layer_size\n",
    "        , generator_upsample\n",
    "        , generator_conv_filters\n",
    "        , generator_conv_kernel_size\n",
    "        , generator_conv_strides\n",
    "        , z_dim\n",
    "        , batch_size\n",
    "        ):\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.discriminator_conv_filters = discriminator_conv_filters\n",
    "        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n",
    "        self.discriminator_conv_strides = discriminator_conv_strides\n",
    "\n",
    "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
    "        self.generator_upsample = generator_upsample\n",
    "        self.generator_conv_filters = generator_conv_filters\n",
    "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
    "        self.generator_conv_strides = generator_conv_strides\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers_discriminator = len(discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(generator_conv_filters)\n",
    "        self.batch_size = batch_size \n",
    "        self.weight_init = RandomNormal(mean = 0 ,stddev=0.02)\n",
    "\n",
    "        self.weight_save_dir =\"./weight_wgan/\"\n",
    "        ##Initialize parameter\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def wasserstein(self, y_true, y_pred):\n",
    "        return -K.mean(y_true * y_pred)\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, samples, sample_weight=None):\n",
    "        gradients = K.gradients(y_pred, samples)[0]\n",
    "        gradient_l2_norm = K.sqrt(\n",
    "            K.sum(\n",
    "                K.square(gradients),\n",
    "                axis=list(range(1, len(gradients.shape))\n",
    "                          )\n",
    "            )\n",
    "        )\n",
    "        return K.mean(K.square(1 - gradient_l2_norm))\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        discriminator_input = Input(shape=self.input_dim)\n",
    "        x =discriminator_input\n",
    "\n",
    "        for i in range(self.n_layers_discriminator):\n",
    "            conv_layer = Conv2D(\n",
    "                filters= self.discriminator_conv_filters[i],\n",
    "                kernel_size = self.discriminator_conv_kernel_size[i],\n",
    "                strides = self.discriminator_conv_strides[i],\n",
    "                padding = 'same',\n",
    "                kernel_initializer= self.weight_init \n",
    "            )\n",
    "            x = conv_layer(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Dropout(rate=0.4)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        discriminator_output = Dense(1,activation=None,kernel_initializer=self.weight_init)(x)\n",
    "\n",
    "        self.discriminator = Model(discriminator_input,discriminator_output)\n",
    "\n",
    "        ## discriminator\n",
    "\n",
    "    def generator_model(self):\n",
    "        generator_input = Input(shape=(self.z_dim,))\n",
    "        x = generator_input\n",
    "\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "        x = Dropout(rate=0.4)(x)\n",
    "\n",
    "        for i in range(self.n_layers_generator):\n",
    "            if self.generator_upsample[i] == 2:\n",
    "                x = UpSampling2D()(x)\n",
    "                conv_layer = Conv2D(\n",
    "                    filters = self.generator_conv_filters[i],\n",
    "                    kernel_size = self.generator_conv_kernel_size[i],\n",
    "                    padding = 'same'\n",
    "                )\n",
    "                x = conv_layer(x)\n",
    "            else:\n",
    "                conv_layer = Conv2DTranspose(\n",
    "                    filters = self.generator_conv_filters[i],\n",
    "                    kernel_size = self.generator_conv_kernel_size[i],\n",
    "                    strides = self.generator_conv_strides[i],\n",
    "                    padding = 'same'\n",
    "                )\n",
    "                x = conv_layer(x)\n",
    "            if i < self.n_layers_generator -1:\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Activation('relu')(x)\n",
    "            else:\n",
    "                x = Activation('tanh')(x)\n",
    "        \n",
    "        generator_output = x\n",
    "        self.generator = Model(generator_input,generator_output)\n",
    "\n",
    "    def critic_model(self):\n",
    "        real_img = Input(shape=self.input_dim)\n",
    "        z_disc = Input(shape =(self.z_dim, ))\n",
    "        fake_img = self.generator(z_disc)\n",
    "        fake = self.discriminator(fake_img)\n",
    "        valid = self.discriminator(real_img)\n",
    "        # fake = fake_image's discriminator value\n",
    "        # valid = real_image's discriminator value\n",
    "\n",
    "        interpolated_img = RandomWeightedAverage(self.batch_size)([real_img,fake_img])\n",
    "        validity_interpolated =self.discriminator(interpolated_img)\n",
    "        # validity_interpolated = interpolate's image discimiator value\n",
    "        self.gp_loss = partial(self.gradient_penalty_loss, samples = interpolated_img)\n",
    "        \n",
    "        self.critic = Model(inputs=[real_img, z_disc], outputs=[\n",
    "                                  valid, fake, validity_interpolated])\n",
    "\n",
    "    def generator_discrimin_model(self):\n",
    "        model_input = Input(shape=(self.z_dim,))\n",
    "        model_output = self.discriminator(self.generator(model_input))\n",
    "        self.generator_discrimin = Model(model_input,model_output)\n",
    "        # In this model = Random noise -> generate -> discriminate \n",
    "        # Mean : generator's fake image discriminating\n",
    "     \n",
    "    def generator_compile(self):\n",
    "        self.generator.compile(\n",
    "            optimizer=RMSprop(lr=0.0008),\n",
    "            loss=self.wasserstein\n",
    "        )\n",
    "        # Don't Used\n",
    "\n",
    "    def critic_compile(self):\n",
    "        self.generator.trainable = False\n",
    "        self.discriminator.trainable = True\n",
    "        # In Critic, generator not train, Only discriminator train\n",
    "        # gp in partial function in critic model\n",
    "        self.critic.compile(\n",
    "            loss=[self.wasserstein, self.wasserstein, self.gp_loss],\n",
    "            optimizer=Adam(lr=0.0008, beta_1=0.5),\n",
    "            loss_weights=[1, 1, 10]\n",
    "        )\n",
    "        # critic have 3 output, valid,fake,interpolate,\n",
    "        # interpolate image nedd gp loss, loss_weight = 10\n",
    "\n",
    "    def generator_discrimin_compile(self):\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "        # model for generator, so discriminator training = False\n",
    "        self.generator_discrimin.compile(\n",
    "            optimizer=RMSprop(lr=0.0004),\n",
    "            loss=self.wasserstein,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    def critic_train(self,x_train,batch_size):\n",
    "        valid = np.ones((batch_size, 1), dtype=np.float32)\n",
    "        fake = -np.ones((batch_size,1), dtype=np.float32)\n",
    "        dummy = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        true_image = x_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return self.critic.train_on_batch([true_image,noise],[valid,fake,dummy])\n",
    "\n",
    "    def generator_train(self,batch_size):\n",
    "        valid = np.ones((batch_size, 1), dtype=np.float32)\n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return self.generator_discrimin.train_on_batch(noise, valid)\n",
    "        # In model to 1(generator train)\n",
    "\n",
    "    def train(self,x_train,epoch):\n",
    "        for i in range(epoch):\n",
    "            for _ in range(5):\n",
    "                d_loss = self.critic_train(x_train, self.batch_size)\n",
    "\n",
    "                ## train critic\n",
    "            g_loss = self.generator_train(self.batch_size)\n",
    "            ## train generator\n",
    "            print(i, end=\" \")\n",
    "        \n",
    "    \n",
    "            if i%100 == 0:\n",
    "                show_random_image(self)\n",
    "                self.save_weight()\n",
    "                print(i,d_loss,g_loss)\n",
    "        return (i,d_loss,g_loss)\n",
    "\n",
    "    def _build(self):\n",
    "        self.discriminator_model()\n",
    "        self.generator_model()\n",
    "        self.critic_model()\n",
    "        self.generator_discrimin_model()\n",
    "\n",
    "        self.critic_compile()\n",
    "        self.generator_discrimin_compile()\n",
    "\n",
    "    def save_weight(self):\n",
    "        self.generator_discrimin.save_weights(self.weight_save_dir + \"generator.h5\")\n",
    "        self.critic.save_weights(self.weight_save_dir + \"critic.h5\")\n",
    "        print(\"weight save\")\n",
    "\n",
    "    def load_weight(self):\n",
    "        try:\n",
    "            self.generator_discrimin.load_weights(self.weight_save_dir + \"generator.h5\")\n",
    "            self.critic.load_weights(self.weight_save_dir + \"critic.h5\")\n",
    "            \n",
    "        except:\n",
    "            self.save_weight()\n",
    "            self.load_weight()\n",
    "\n",
    "        print(\"weight loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (64,64,3)\n",
    "        , discriminator_conv_filters = [64,128,256,512]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,2]\n",
    "        , generator_initial_dense_layer_size = (4, 4, 512)\n",
    "        , generator_upsample = [1,1, 1, 1]\n",
    "        , generator_conv_filters = [256,128, 64,3]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [2,2, 2, 2]\n",
    "        , z_dim = 100\n",
    "        , batch_size = 64\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "data = None\n",
    "\n",
    "def camel_data():\n",
    "    data = np.load(\"/home/sjo506/Gan_practice/test/camel/camel.npy\")\n",
    "    data = data/255.0\n",
    "    data = np.reshape(data,(len(data),28,28,1))\n",
    "    return data\n",
    "\n",
    "def caleba_data():\n",
    "    DATA_PATH = '/home/sjo506/Gan_practice/test/celeba'\n",
    "    BATCH_SIZE = 64*4\n",
    "    data_gen = ImageDataGenerator(rescale=1./255) \n",
    "    data_flow = data_gen.flow_from_directory(DATA_PATH,\n",
    "    target_size = [64,64],\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle= True,\n",
    "    class_mode='input',\n",
    "    subset='training'\n",
    "        )\n",
    "    return data_flow\n",
    "\n",
    "def show_random_image(trainer):\n",
    "    znew = np.random.normal(0, 1, size=(1, gan.z_dim))\n",
    "    znew_img = trainer.generator.predict(znew)[0]\n",
    "    znew_img = np.clip((0.5*(znew_img+1)),0,1)\n",
    "    plt.imshow(znew_img)\n",
    "    plt.show()\n",
    "\n",
    "data = caleba_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight loaded\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjo506/.local/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/sjo506/.local/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight save\n",
      "0 [nan, nan, nan, nan] [nan, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjo506/.local/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight save\n",
      "100 [nan, nan, nan, nan] [nan, 0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d34264b518b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_random_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0bc542652375>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, epoch)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m## train critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0bc542652375>\u001b[0m in \u001b[0;36mcritic_train\u001b[0;34m(self, x_train, batch_size)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerator_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.load_weight()\n",
    "#for data_g in data:\n",
    "train,label = next(data)\n",
    "show_random_image(gan)\n",
    "loss = gan.train(train,1000)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_image(gan)\n",
    "loss = gan.train(train,1000)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
